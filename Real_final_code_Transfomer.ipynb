{"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","\n","# Define a simple Dataset class\n","class CustomDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx], self.labels[idx]\n","\n","# Transformer model definition\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_dim, model_dim, num_heads, num_layers, num_classes):\n","        super(TransformerModel, self).__init__()\n","\n","        self.embedding = nn.Linear(input_dim, model_dim)\n","        self.positional_encoding = nn.Parameter(torch.zeros(1, 100, model_dim))\n","\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","\n","        self.fc = nn.Linear(model_dim, num_classes)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n","        encoded = self.transformer_encoder(embedded)\n","        output = self.fc(encoded.mean(dim=1))  # Global average pooling\n","        return output\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Dummy data\n","    input_dim = 10\n","    model_dim = 64\n","    num_heads = 4\n","    num_layers = 2\n","    num_classes = 3\n","\n","    data = np.random.rand(100, 10).astype(np.float32)  # 100 samples, 10 features each\n","    labels = np.random.randint(0, num_classes, size=(100,))\n","\n","    dataset = CustomDataset(data, labels)\n","    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","    model = TransformerModel(input_dim, model_dim, num_heads, num_layers, num_classes)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Training loop\n","    for epoch in range(10):\n","        for batch_data, batch_labels in dataloader:\n","            batch_data = batch_data.permute(1, 0, 2)  # Transformer expects seq_len x batch_size x features\n","            batch_labels = batch_labels\n","\n","            optimizer.zero_grad()\n","            outputs = model(batch_data)\n","            loss = criterion(outputs, batch_labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"],"metadata":{"id":"hmdrvqJ4ovrg"},"id":"hmdrvqJ4ovrg","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}